{
  "hash": "c4c26c8549ebe4aefa1347bc22bc3d7f",
  "result": {
    "markdown": "---\ntitle: \"23 - Tidytext and sentiment analysis\"\nauthor:\n  - name: Leonardo Collado Torres\n    url: http://lcolladotor.github.io/\n    affiliations:\n      - id: libd\n        name: Lieber Institute for Brain Development\n        url: https://libd.org/\n      - id: jhsph\n        name: Johns Hopkins Bloomberg School of Public Health Department of Biostatistics\n        url: https://publichealth.jhu.edu/departments/biostatistics\ndescription: \"Introduction to tidytext and sentiment analysis\"\ncategories: [module 5, week 7, tidyverse, tidytext, sentiment analysis]\n---\n\n\n*This lecture, as the rest of the course, is adapted from the version [Stephanie C. Hicks](https://www.stephaniehicks.com/) designed and maintained in 2021 and 2022. Check the recent changes to this file through the [GitHub history](https://github.com/lcolladotor/jhustatcomputing/commits/main/posts/23-working-with-text-sentiment-analysis/index.qmd).*\n\n<!-- Add interesting quote -->\n\n# Pre-lecture materials\n\n### Acknowledgements\n\nMaterial for this lecture was borrowed and adopted from\n\n-   [Text mining with R: A Tidy Approach](https://www.tidytextmining.com/) from Julia Silge and David Robinson which uses the [`tidytext`](https://github.com/juliasilge/tidytext) R package\n-   [Supervised Machine Learning for Text Analsyis in R](https://smltar.com/preface.html) from Emil Hvitfeldt, Julia Silge\n-   You might find this text sentiment analysis by David Robinson interesting as an example use case of the tools we will learn today: <http://varianceexplained.org/r/trump-tweets/>\n\n# Learning objectives\n\n::: callout-note\n# Learning objectives\n\n**At the end of this lesson you will:**\n\n-   Learn about what is is meant by \"tidy text\" data\n-   Know the fundamentals of the `tidytext` R package to create tidy text data\n-   Know the fundamentals of sentiment analysis\n:::\n\n# Motivation\n\nAnalyzing text data such as Twitter content, books or news articles is commonly performed in data science.\n\nIn this lecture, we will be asking the following questions:\n\n1.  Which are the **most commonly used words** from Jane Austen's novels?\n2.  Which are the **most positive** or **negative words**?\n3.  How does the **sentiment** (e.g. positive vs negative) of the text change across each novel?\n\n![](https://images-na.ssl-images-amazon.com/images/I/A1YUH7-W5AL.jpg){preview=\"TRUE\"}\n\n\\[[image source](https://images-na.ssl-images-amazon.com/images/I/A1YUH7-W5AL.jpg)\\]\n\nTo answer these questions, we will need to learn about a few things. Specifically,\n\n1.  How to **convert words in documents** to a **tidy text** format using the `tidytext` R package.\n2.  A little bit about [sentiment analysis](https://www.tidytextmining.com/sentiment.html).\n\n# Tidy text\n\nIn previous lectures, you have learned about the **tidy data principles** and the `tidyverse` R packages as a way to make handling data easier and more effective.\n\nThese packages depend on **data being formatted in a particular way**.\n\nThe idea with tidy text is to **treat text as data frames of individual words** and **apply the same tidy data principles** to make text mining tasks easier and consistent with already developed tools.\n\nFirst let's recall what a **tidy** data format means.\n\n### What is a **tidy** format?\n\nFirst, the [tidyverse](https://www.tidyverse.org) is\n\n> \"an opinionated collection of R packages designed for data science. All packages share an underlying philosophy and common APIs.\"\n\nAnother way of putting it is that it is a **set of packages** that are useful specifically for data manipulation, exploration and visualization **with a common philosophy**.\n\n### What is this common philosphy?\n\nThe common philosophy is called **\"tidy\" data**.\n\nIt is a standard way of mapping the meaning of a dataset to its structure.\n\nIn **tidy** data:\n\n-   Each variable forms a column.\n-   Each observation forms a row.\n-   Each type of observational unit forms a table.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](http://r4ds.had.co.nz/images/tidy-1.png){width=95%}\n:::\n:::\n\n\n\\[[img source](http://r4ds.had.co.nz)\\]\n\nWorking with **tidy data is useful** because it creates a structured way of organizing data values within a data set.\n\nThis makes the data analysis process more efficient and simplifies the development of data analysis tools that work together.\n\nIn this way, you can focus on the problem you are investigating, rather than the uninteresting logistics of data.\n\n### What is a **tidy text** format?\n\nWhen dealing with **text** data, the **tidy text** format is defined as a table **with one-token-per-row**, where a **token** is a meaningful unit of text (e.g. a word, pair of words, sentence, paragraph, etc).\n\nUsing a **given set of token**, we can **tokenize** text, or **split the text into the defined tokens of interest along the rows**.\n\nWe will learn more about how to do this using functions in the [`tidytext`](https://github.com/juliasilge/tidytext) R package.\n\nIn contrast, other data structures that are commonly used to store text data in text mining applications:\n\n-   **string**: text can, of course, be stored as strings, i.e., character vectors, within R, and often text data is first read into memory in this form.\n-   **corpus**: these types of objects typically contain raw strings annotated with additional metadata and details.\n-   **document-term matrix**: This is a sparse matrix describing a collection (i.e., a corpus) of documents with one row for each document and one column for each term. The value in the matrix is typically word count.\n\nI won't describing these other formats in greater detail, but encourage you to read about them if interested in this topic.\n\n### Why is this format useful?\n\nOne of the biggest advantages of transforming text data to the tidy text format is that it allows data to transition smoothly between other packages that adhere to the `tidyverse` framework (e.g. `ggplot2`, `dplyr`, etc).\n\n\n::: {.cell}\n::: {.cell-output-display}\n![A flowchart of a typical text analysis using tidy data principles.](https://www.tidytextmining.com/images/tmwr_0101.png){width=90%}\n:::\n:::\n\n\n\\[[image source](https://www.tidytextmining.com/images/tmwr_0101.png)\\]\n\nIn addition, a user can transition between the tidy text format for e.g data visualization with `ggplot2`, but then also convert data to other data structures (e.g. document-term matrix) that is commonly used in machine learning applications.\n\n### How does it work?\n\nThe main workhorse function in the `tidytext` R package to tokenize text a data frame is the `unnest_tokens(tbl, output, input)` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n?unnest_tokens\n```\n:::\n\n\nIn addition to the tibble or data frame (`tbl`), the function needs two basic arguments:\n\n1.  `output` or the output column name that will be created (e.g. string) as the text is unnested into it\n2.  `input` or input column name that the text comes from and gets split\n\nLet's try out the `unnest_tokens()` function using the first paragraph in the preface of Roger Peng's [R Programming for Data Science](https://leanpub.com/rprogramming) book.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Preface from R Programming for Data Science](../../images/peng_preface.png){width=90%}\n:::\n:::\n\n\nTo make this easier, I typed this text into a vector of character strings: one string per sentence.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeng_preface <-\n    c(\n        \"I started using R in 1998 when I was a college undergraduate working on my senior thesis.\",\n        \"The version was 0.63.\",\n        \"I was an applied mathematics major with a statistics concentration and I was working with Dr. Nicolas Hengartner on an analysis of word frequencies in classic texts (Shakespeare, Milton, etc.).\",\n        \"The idea was to see if we could identify the authorship of each of the texts based on how frequently they used certain words.\",\n        \"We downloaded the data from Project Gutenberg and used some basic linear discriminant analysis for the modeling.\",\n        \"The work was eventually published and was my first ever peer-reviewed publication.\",\n        \"I guess you could argue it was my first real 'data science' experience.\"\n    )\n\npeng_preface\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"I started using R in 1998 when I was a college undergraduate working on my senior thesis.\"                                                                                                        \n[2] \"The version was 0.63.\"                                                                                                                                                                            \n[3] \"I was an applied mathematics major with a statistics concentration and I was working with Dr. Nicolas Hengartner on an analysis of word frequencies in classic texts (Shakespeare, Milton, etc.).\"\n[4] \"The idea was to see if we could identify the authorship of each of the texts based on how frequently they used certain words.\"                                                                    \n[5] \"We downloaded the data from Project Gutenberg and used some basic linear discriminant analysis for the modeling.\"                                                                                 \n[6] \"The work was eventually published and was my first ever peer-reviewed publication.\"                                                                                                               \n[7] \"I guess you could argue it was my first real 'data science' experience.\"                                                                                                                          \n```\n:::\n:::\n\n\nTurns out Roger performed a similar analysis as an undergraduate student!\n\nHe goes to say that back then no one was using R (but a little bit of something called S-PLUS), so I can only imagine how different it was to accomplish a task like the one we are going to do today compared to when he was an undergraduate.\n\nNext, we load a few R packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(tidytext) ## needs to be installed\nlibrary(janeaustenr) ## needs to be installed\n```\n:::\n\n\nThen, we use the `tibble()` function to construct a data frame with two columns: one counting the line number and one from the character strings in `peng_preface`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeng_preface_df <- tibble(\n    line = 1:7,\n    text = peng_preface\n)\npeng_preface_df\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 7 × 2\n   line text                                                                    \n  <int> <chr>                                                                   \n1     1 I started using R in 1998 when I was a college undergraduate working on…\n2     2 The version was 0.63.                                                   \n3     3 I was an applied mathematics major with a statistics concentration and …\n4     4 The idea was to see if we could identify the authorship of each of the …\n5     5 We downloaded the data from Project Gutenberg and used some basic linea…\n6     6 The work was eventually published and was my first ever peer-reviewed p…\n7     7 I guess you could argue it was my first real 'data science' experience. \n```\n:::\n:::\n\n\n### Text Mining and Tokens\n\nNext, we will use the `unnest_tokens()` function where we will call the output column to be created `word` and the input column `text` from the `peng_preface_df`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeng_token <-\n    peng_preface_df %>%\n    unnest_tokens(\n        output = word,\n        input = text,\n        token = \"words\"\n    )\n\npeng_token %>%\n    head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 2\n   line word   \n  <int> <chr>  \n1     1 i      \n2     1 started\n3     1 using  \n4     1 r      \n5     1 in     \n6     1 1998   \n```\n:::\n\n```{.r .cell-code}\npeng_token %>%\n    tail()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 2\n   line word      \n  <int> <chr>     \n1     7 my        \n2     7 first     \n3     7 real      \n4     7 data      \n5     7 science   \n6     7 experience\n```\n:::\n:::\n\n\nThe argument `token=\"words\"` **defines the unit for tokenization**.\n\nThe default is `\"words\"`, but there are lots of other options.\n\n::: callout-tip\n### Example\n\nWe could tokenize by `\"characters\"`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeng_preface_df %>%\n    unnest_tokens(word,\n        text,\n        token = \"characters\"\n    ) %>%\n    head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 2\n   line word \n  <int> <chr>\n1     1 i    \n2     1 s    \n3     1 t    \n4     1 a    \n5     1 r    \n6     1 t    \n```\n:::\n:::\n\n:::\n\nor something called [ngrams](https://en.wikipedia.org/wiki/N-gram), which is defined by Wikipedia as a *\"contiguous sequence of n items from a given sample of text or speech\"*\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeng_preface_df %>%\n    unnest_tokens(word,\n        text,\n        token = \"ngrams\",\n        n = 3\n    ) %>%\n    head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 2\n   line word           \n  <int> <chr>          \n1     1 i started using\n2     1 started using r\n3     1 using r in     \n4     1 r in 1998      \n5     1 in 1998 when   \n6     1 1998 when i    \n```\n:::\n:::\n\n\nAnother option is to use the `character_shingles` option, which is similar to tokenizing like `ngrams`, except the units are characters instead of words.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeng_preface_df %>%\n    unnest_tokens(word,\n        text,\n        token = \"character_shingles\",\n        n = 4\n    ) %>%\n    head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 2\n   line word \n  <int> <chr>\n1     1 ista \n2     1 star \n3     1 tart \n4     1 arte \n5     1 rted \n6     1 tedu \n```\n:::\n:::\n\n\nYou can also **create custom functions** for tokenization.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeng_preface_df %>%\n    unnest_tokens(word,\n        text,\n        token = stringr::str_split,\n        pattern = \" \"\n    ) %>%\n    head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 2\n   line word   \n  <int> <chr>  \n1     1 i      \n2     1 started\n3     1 using  \n4     1 r      \n5     1 in     \n6     1 1998   \n```\n:::\n:::\n\n\n::: callout-note\n### Question\n\nLet's tokenize the first four sentences of [Amanda Gorman's *The Hill We Climb*](https://www.nytimes.com/2021/01/19/books/amanda-gorman-inauguration-hill-we-climb.html) by words.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngorman_hill_we_climb <-\n    c(\n        \"When day comes we ask ourselves, where can we find light in this neverending shade?\",\n        \"The loss we carry, a sea we must wade.\",\n        \"We’ve braved the belly of the beast, we’ve learned that quiet isn’t always peace and the norms and notions of what just is, isn’t always justice.\",\n        \"And yet the dawn is ours before we knew it, somehow we do it, somehow we’ve weathered and witnessed a nation that isn’t broken but simply unfinished.\"\n    )\n\nhill_df <- tibble(\n    line = seq_along(gorman_hill_we_climb),\n    text = gorman_hill_we_climb\n)\nhill_df\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 2\n   line text                                                                    \n  <int> <chr>                                                                   \n1     1 When day comes we ask ourselves, where can we find light in this nevere…\n2     2 The loss we carry, a sea we must wade.                                  \n3     3 We’ve braved the belly of the beast, we’ve learned that quiet isn’t alw…\n4     4 And yet the dawn is ours before we knew it, somehow we do it, somehow w…\n```\n:::\n\n```{.r .cell-code}\n### try it out\n\nhill_df %>%\n    unnest_tokens(\n        output = wordsforfun,\n        input = text,\n        token = \"words\"\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 77 × 2\n    line wordsforfun\n   <int> <chr>      \n 1     1 when       \n 2     1 day        \n 3     1 comes      \n 4     1 we         \n 5     1 ask        \n 6     1 ourselves  \n 7     1 where      \n 8     1 can        \n 9     1 we         \n10     1 find       \n# ℹ 67 more rows\n```\n:::\n:::\n\n:::\n\n# Example: text from works of Jane Austen\n\nWe will use the text from six published novels from Jane Austen, which are available in the [`janeaustenr`](https://cran.r-project.org/web/packages/janeaustenr/index.html) R package. The [authors](https://www.tidytextmining.com/tidytext.html#tidyausten) describe the format:\n\n> \"The package provides the text in a one-row-per-line format, where a line is this context is analogous to a literal printed line in a physical book.\n>\n> The package contains:\n>\n> -   `sensesensibility`: Sense and Sensibility, published in 1811\n> -   `prideprejudice`: Pride and Prejudice, published in 1813\n> -   `mansfieldpark`: Mansfield Park, published in 1814\n> -   `emma`: Emma, published in 1815\n> -   `northangerabbey`: Northanger Abbey, published posthumously in 1818\n> -   `persuasion`: Persuasion, also published posthumously in 1818\n>\n> There is also a function `austen_books()` that returns a tidy data frame of all 6 novels.\"\n\nLet's load in the text from `prideprejudice` and look at how the data are stored.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(janeaustenr)\nhead(prideprejudice, 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"PRIDE AND PREJUDICE\"                                                        \n [2] \"\"                                                                           \n [3] \"By Jane Austen\"                                                             \n [4] \"\"                                                                           \n [5] \"\"                                                                           \n [6] \"\"                                                                           \n [7] \"Chapter 1\"                                                                  \n [8] \"\"                                                                           \n [9] \"\"                                                                           \n[10] \"It is a truth universally acknowledged, that a single man in possession\"    \n[11] \"of a good fortune, must be in want of a wife.\"                              \n[12] \"\"                                                                           \n[13] \"However little known the feelings or views of such a man may be on his\"     \n[14] \"first entering a neighbourhood, this truth is so well fixed in the minds\"   \n[15] \"of the surrounding families, that he is considered the rightful property\"   \n[16] \"of some one or other of their daughters.\"                                   \n[17] \"\"                                                                           \n[18] \"\\\"My dear Mr. Bennet,\\\" said his lady to him one day, \\\"have you heard that\"\n[19] \"Netherfield Park is let at last?\\\"\"                                         \n[20] \"\"                                                                           \n```\n:::\n:::\n\n\nWe see each line is in a character vector with elements of about 70 characters.\n\nSimilar to what we did above with Roger's preface, we can\n\n-   Turn the text of character strings into a data frame and then\n-   Convert it into a one-row-per-line dataframe using the `unnest_tokens()` function\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp_book_df <- tibble(text = prideprejudice)\n\npp_book_df %>%\n    unnest_tokens(\n        output = word,\n        input = text,\n        token = \"words\"\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 122,204 × 1\n   word     \n   <chr>    \n 1 pride    \n 2 and      \n 3 prejudice\n 4 by       \n 5 jane     \n 6 austen   \n 7 chapter  \n 8 1        \n 9 it       \n10 is       \n# ℹ 122,194 more rows\n```\n:::\n:::\n\n\nWe can also divide it by paragraphs:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp <- pp_book_df %>%\n    unnest_tokens(\n        output = paragraph,\n        input = text,\n        token = \"paragraphs\"\n    )\ntmp\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10,721 × 1\n   paragraph                                                                    \n   <chr>                                                                        \n 1 \"pride and prejudice\"                                                        \n 2 \"by jane austen\"                                                             \n 3 \"chapter 1\"                                                                  \n 4 \"it is a truth universally acknowledged, that a single man in possession\"    \n 5 \"of a good fortune, must be in want of a wife.\"                              \n 6 \"however little known the feelings or views of such a man may be on his\"     \n 7 \"first entering a neighbourhood, this truth is so well fixed in the minds\"   \n 8 \"of the surrounding families, that he is considered the rightful property\"   \n 9 \"of some one or other of their daughters.\"                                   \n10 \"\\\"my dear mr. bennet,\\\" said his lady to him one day, \\\"have you heard that\"\n# ℹ 10,711 more rows\n```\n:::\n:::\n\n\nWe can extract a particular element from the tibble\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp[3, 1]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n  paragraph\n  <chr>    \n1 chapter 1\n```\n:::\n:::\n\n\n::: callout-tip\n### Note\n\nWhat you name the output column, e.g. `paragraph` in this case, doesn't affect it, it's just good to give it a consistent name.\n:::\n\nWe could also divide it by sentence:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp_book_df %>%\n    unnest_tokens(\n        output = sentence,\n        input = text,\n        token = \"sentences\"\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 15,545 × 1\n   sentence                                                                  \n   <chr>                                                                     \n 1 \"pride and prejudice\"                                                     \n 2 \"by jane austen\"                                                          \n 3 \"chapter 1\"                                                               \n 4 \"it is a truth universally acknowledged, that a single man in possession\" \n 5 \"of a good fortune, must be in want of a wife.\"                           \n 6 \"however little known the feelings or views of such a man may be on his\"  \n 7 \"first entering a neighbourhood, this truth is so well fixed in the minds\"\n 8 \"of the surrounding families, that he is considered the rightful property\"\n 9 \"of some one or other of their daughters.\"                                \n10 \"\\\"my dear mr.\"                                                           \n# ℹ 15,535 more rows\n```\n:::\n:::\n\n\n::: callout-tip\n### Note\n\nThis is tricked by terms like \"Mr.\" and \"Mrs.\"\n:::\n\nOne neat trick is that we can unnest by two layers:\n\n1.  paragraph and then\n2.  word\n\nThis lets us keep track of **which paragraph is which**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparagraphs <-\n    pp_book_df %>%\n    unnest_tokens(\n        output = paragraph,\n        input = text,\n        token = \"paragraphs\"\n    ) %>%\n    mutate(paragraph_number = row_number())\n\nparagraphs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10,721 × 2\n   paragraph                                                    paragraph_number\n   <chr>                                                                   <int>\n 1 \"pride and prejudice\"                                                       1\n 2 \"by jane austen\"                                                            2\n 3 \"chapter 1\"                                                                 3\n 4 \"it is a truth universally acknowledged, that a single man …                4\n 5 \"of a good fortune, must be in want of a wife.\"                             5\n 6 \"however little known the feelings or views of such a man m…                6\n 7 \"first entering a neighbourhood, this truth is so well fixe…                7\n 8 \"of the surrounding families, that he is considered the rig…                8\n 9 \"of some one or other of their daughters.\"                                  9\n10 \"\\\"my dear mr. bennet,\\\" said his lady to him one day, \\\"ha…               10\n# ℹ 10,711 more rows\n```\n:::\n:::\n\n\n::: callout-tip\n### Note\n\nWe use `mutate()` to annotate a paragraph number quantity to keep track of paragraphs in the original format.\n:::\n\nAfter tokenizing by paragraph, we can then tokenzie by word:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparagraphs %>%\n    unnest_tokens(\n        output = word,\n        input = paragraph\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 122,204 × 2\n   paragraph_number word     \n              <int> <chr>    \n 1                1 pride    \n 2                1 and      \n 3                1 prejudice\n 4                2 by       \n 5                2 jane     \n 6                2 austen   \n 7                3 chapter  \n 8                3 1        \n 9                4 it       \n10                4 is       \n# ℹ 122,194 more rows\n```\n:::\n:::\n\n\nWe notice there are many what are called **stop words** (\"the\", \"of\", \"to\", and so forth in English).\n\nOften in text analysis, we will want to **remove stop words** because stop words are words that are not useful for an analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(stop_words)\n\ntable(stop_words$lexicon)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n    onix    SMART snowball \n     404      571      174 \n```\n:::\n\n```{.r .cell-code}\nstop_words %>%\n    head(n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 × 2\n   word        lexicon\n   <chr>       <chr>  \n 1 a           SMART  \n 2 a's         SMART  \n 3 able        SMART  \n 4 about       SMART  \n 5 above       SMART  \n 6 according   SMART  \n 7 accordingly SMART  \n 8 across      SMART  \n 9 actually    SMART  \n10 after       SMART  \n```\n:::\n:::\n\n\nWe can remove stop words (kept in the `tidytext` dataset `stop_words`) with an `anti_join(x,y)` (return all rows from `x` without a match in `y`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwords_by_paragraph <-\n    paragraphs %>%\n    unnest_tokens(\n        output = word,\n        input = paragraph\n    ) %>%\n    anti_join(stop_words)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining with `by = join_by(word)`\n```\n:::\n\n```{.r .cell-code}\nwords_by_paragraph\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 37,246 × 2\n   paragraph_number word        \n              <int> <chr>       \n 1                1 pride       \n 2                1 prejudice   \n 3                2 jane        \n 4                2 austen      \n 5                3 chapter     \n 6                3 1           \n 7                4 truth       \n 8                4 universally \n 9                4 acknowledged\n10                4 single      \n# ℹ 37,236 more rows\n```\n:::\n:::\n\n\nBecause we have stored our data in a tidy dataset, we can use `tidyverse` packages for exploratory data analysis.\n\nFor example, here we use `dplyr`'s `count()` function to find the most common words in the book\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwords_by_paragraph %>%\n    count(word, sort = TRUE) %>%\n    head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 2\n  word          n\n  <chr>     <int>\n1 elizabeth   597\n2 darcy       373\n3 bennet      294\n4 miss        283\n5 jane        264\n6 bingley     257\n```\n:::\n:::\n\n\nThen use `ggplot2` to plot the most commonly used words from the book.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwords_by_paragraph %>%\n    count(word, sort = TRUE) %>%\n    filter(n > 150) %>%\n    mutate(word = fct_reorder(word, n)) %>%\n    ggplot(aes(word, n)) +\n    geom_col() +\n    xlab(NULL) +\n    coord_flip()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\nWe can also do this for all of her books using the `austen_books()` object\n\n\n::: {.cell}\n\n```{.r .cell-code}\nausten_books() %>%\n    head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 2\n  text                    book               \n  <chr>                   <fct>              \n1 \"SENSE AND SENSIBILITY\" Sense & Sensibility\n2 \"\"                      Sense & Sensibility\n3 \"by Jane Austen\"        Sense & Sensibility\n4 \"\"                      Sense & Sensibility\n5 \"(1811)\"                Sense & Sensibility\n6 \"\"                      Sense & Sensibility\n```\n:::\n:::\n\n\nWe can do some data wrangling that keep tracks of the line number and chapter (using a regex) to find where all the chapters are.\n\n\n::: {.cell}\n\n```{.r .cell-code}\noriginal_books <-\n    austen_books() %>%\n    group_by(book) %>%\n    mutate(\n        linenumber = row_number(),\n        chapter = cumsum(\n            str_detect(text,\n                pattern = regex(\n                    pattern = \"^chapter [\\\\divxlc]\",\n                    ignore_case = TRUE\n                )\n            )\n        )\n    ) %>%\n    ungroup()\n\noriginal_books\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 73,422 × 4\n   text                    book                linenumber chapter\n   <chr>                   <fct>                    <int>   <int>\n 1 \"SENSE AND SENSIBILITY\" Sense & Sensibility          1       0\n 2 \"\"                      Sense & Sensibility          2       0\n 3 \"by Jane Austen\"        Sense & Sensibility          3       0\n 4 \"\"                      Sense & Sensibility          4       0\n 5 \"(1811)\"                Sense & Sensibility          5       0\n 6 \"\"                      Sense & Sensibility          6       0\n 7 \"\"                      Sense & Sensibility          7       0\n 8 \"\"                      Sense & Sensibility          8       0\n 9 \"\"                      Sense & Sensibility          9       0\n10 \"CHAPTER 1\"             Sense & Sensibility         10       1\n# ℹ 73,412 more rows\n```\n:::\n:::\n\n\nFinally, we can restructure it to a one-token-per-row format using the `unnest_tokens()` function and remove stop words using the `anti_join()` function in `dplyr`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_books <- original_books %>%\n    unnest_tokens(word, text) %>%\n    anti_join(stop_words)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining with `by = join_by(word)`\n```\n:::\n\n```{.r .cell-code}\ntidy_books\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 217,609 × 4\n   book                linenumber chapter word       \n   <fct>                    <int>   <int> <chr>      \n 1 Sense & Sensibility          1       0 sense      \n 2 Sense & Sensibility          1       0 sensibility\n 3 Sense & Sensibility          3       0 jane       \n 4 Sense & Sensibility          3       0 austen     \n 5 Sense & Sensibility          5       0 1811       \n 6 Sense & Sensibility         10       1 chapter    \n 7 Sense & Sensibility         10       1 1          \n 8 Sense & Sensibility         13       1 family     \n 9 Sense & Sensibility         13       1 dashwood   \n10 Sense & Sensibility         13       1 settled    \n# ℹ 217,599 more rows\n```\n:::\n:::\n\n\nHere are the most commonly used words across all of Jane Austen's books.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_books %>%\n    count(word, sort = TRUE) %>%\n    filter(n > 600) %>%\n    mutate(word = fct_reorder(word, n)) %>%\n    ggplot(aes(word, n)) +\n    geom_col() +\n    xlab(NULL) +\n    coord_flip()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\n# Sentiment Analysis\n\nIn the previous section, we explored the **tidy text** format and showed how we can calculate things such as word frequency.\n\nNext, we are going to look at something called **opinion mining** or **sentiment analysis**. The [tidytext authors](https://www.tidytextmining.com/sentiment.html) write:\n\n> *\"When human readers approach a text, we use our understanding of the emotional intent of words to infer whether a section of text is positive or negative, or perhaps characterized by some other more nuanced emotion like surprise or disgust. We can use the tools of text mining to approach the emotional content of text programmatically, as shown in the figure below\"*\n\n\n::: {.cell}\n::: {.cell-output-display}\n![A flowchart of a typical text analysis that uses tidytext for sentiment analysis.](https://www.tidytextmining.com/images/tmwr_0201.png){width=90%}\n:::\n:::\n\n\n\\[[image source](https://www.tidytextmining.com/images/tmwr_0201.png)\\]\n\n> *\"One way to analyze the sentiment of a text is to consider the text as a combination of its individual words and the sentiment content of the whole text as the sum of the sentiment content of the individual words. This isn't the only way to approach sentiment analysis, but it is an often-used approach, and an approach that naturally takes advantage of the tidy tool ecosystem.\"*\n\nLet's try using sentiment analysis on the Jane Austen books.\n\n## The `sentiments` dataset\n\nInside the `tidytext` package are several **sentiment lexicons**. A few things to note:\n\n-   The lexicons are based on unigrams (single words)\n-   The lexicons contain many English words and the words are assigned scores for positive/negative sentiment, and also possibly emotions like joy, anger, sadness, and so forth\n\nYou can use the `get_sentiments()` function to extract a specific lexicon.\n\nThe `nrc` lexicon **categorizes words into multiple categories** of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_sentiments(\"nrc\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 13,872 × 2\n   word        sentiment\n   <chr>       <chr>    \n 1 abacus      trust    \n 2 abandon     fear     \n 3 abandon     negative \n 4 abandon     sadness  \n 5 abandoned   anger    \n 6 abandoned   fear     \n 7 abandoned   negative \n 8 abandoned   sadness  \n 9 abandonment anger    \n10 abandonment fear     \n# ℹ 13,862 more rows\n```\n:::\n:::\n\n\nThe `bing` lexicon **categorizes words in a binary fashion** into positive and negative categories\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_sentiments(\"bing\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6,786 × 2\n   word        sentiment\n   <chr>       <chr>    \n 1 2-faces     negative \n 2 abnormal    negative \n 3 abolish     negative \n 4 abominable  negative \n 5 abominably  negative \n 6 abominate   negative \n 7 abomination negative \n 8 abort       negative \n 9 aborted     negative \n10 aborts      negative \n# ℹ 6,776 more rows\n```\n:::\n:::\n\n\nThe `AFINN` lexicon **assigns words with a score that runs between -5 and 5**, with negative scores indicating negative sentiment and positive scores indicating positive sentiment\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_sentiments(\"afinn\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2,477 × 2\n   word       value\n   <chr>      <dbl>\n 1 abandon       -2\n 2 abandoned     -2\n 3 abandons      -2\n 4 abducted      -2\n 5 abduction     -2\n 6 abductions    -2\n 7 abhor         -3\n 8 abhorred      -3\n 9 abhorrent     -3\n10 abhors        -3\n# ℹ 2,467 more rows\n```\n:::\n:::\n\n\nThe authors of the `tidytext` package note:\n\n> *\"How were these sentiment lexicons put together and validated? They were constructed via either crowdsourcing (using, for example, Amazon Mechanical Turk) or by the labor of one of the authors, and were validated using some combination of crowdsourcing again, restaurant or movie reviews, or Twitter data. Given this information, we may hesitate to apply these sentiment lexicons to styles of text dramatically different from what they were validated on, such as narrative fiction from 200 years ago. While it is true that using these sentiment lexicons with, for example, Jane Austen's novels may give us less accurate results than with tweets sent by a contemporary writer, we still can measure the sentiment content for words that are shared across the lexicon and the text.\"*\n\nTwo other caveats:\n\n> *\"Not every English word is in the lexicons because many English words are pretty neutral. It is important to keep in mind that these methods do not take into account qualifiers before a word, such as in\"no good\" or \"not true\"; a lexicon-based method like this is based on unigrams only. For many kinds of text (like the narrative examples below), there are not sustained sections of sarcasm or negated text, so this is not an important effect.\"*\n\nand\n\n> *\"One last caveat is that the size of the chunk of text that we use to add up unigram sentiment scores can have an effect on an analysis. A text the size of many paragraphs can often have positive and negative sentiment averaged out to about zero, while sentence-sized or paragraph-sized text often works better.\"*\n\n### Joining together tidy text data with lexicons\n\nNow that we have our data in a tidy text format AND we have learned about different types of lexicons in application for sentiment analysis, we can **join the words together** using a join function.\n\n::: callout-tip\n### Example\n\nWhat are the most common joy words in the book *Emma*?\n\nHere, we use the `nrc` lexicon and join the `tidy_books` dataset with the `nrc_joy` lexicon using the `inner_join()` function in `dplyr`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnrc_joy <- get_sentiments(\"nrc\") %>%\n    filter(sentiment == \"joy\")\n\ntidy_books %>%\n    filter(book == \"Emma\") %>%\n    inner_join(nrc_joy) %>%\n    count(word, sort = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining with `by = join_by(word)`\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 297 × 2\n   word          n\n   <chr>     <int>\n 1 friend      166\n 2 hope        143\n 3 happy       125\n 4 love        117\n 5 deal         92\n 6 found        92\n 7 happiness    76\n 8 pretty       68\n 9 true         66\n10 comfort      65\n# ℹ 287 more rows\n```\n:::\n:::\n\n:::\n\nWe can do things like investigate how the sentiment of the text changes throughout each of Jane's novels.\n\nHere, we use the `bing` lexicon, find a sentiment score for each word, and then use `inner_join()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_books %>%\n    inner_join(get_sentiments(\"bing\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining with `by = join_by(word)`\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in inner_join(., get_sentiments(\"bing\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 131015 of `x` matches multiple rows in `y`.\nℹ Row 5051 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 44,171 × 5\n   book                linenumber chapter word        sentiment\n   <fct>                    <int>   <int> <chr>       <chr>    \n 1 Sense & Sensibility         16       1 respectable positive \n 2 Sense & Sensibility         18       1 advanced    positive \n 3 Sense & Sensibility         20       1 death       negative \n 4 Sense & Sensibility         21       1 loss        negative \n 5 Sense & Sensibility         25       1 comfortably positive \n 6 Sense & Sensibility         28       1 goodness    positive \n 7 Sense & Sensibility         28       1 solid       positive \n 8 Sense & Sensibility         29       1 comfort     positive \n 9 Sense & Sensibility         30       1 relish      positive \n10 Sense & Sensibility         33       1 steady      positive \n# ℹ 44,161 more rows\n```\n:::\n:::\n\n\nThen, we can **count how many positive and negative words** there are in each section of the books.\n\nWe create an index to help us keep track of where we are in the narrative, which uses integer division, and counts up sections of 80 lines of text.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_books %>%\n    inner_join(get_sentiments(\"bing\")) %>%\n    count(book,\n        index = linenumber %/% 80,\n        sentiment\n    )\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining with `by = join_by(word)`\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in inner_join(., get_sentiments(\"bing\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 131015 of `x` matches multiple rows in `y`.\nℹ Row 5051 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,840 × 4\n   book                index sentiment     n\n   <fct>               <dbl> <chr>     <int>\n 1 Sense & Sensibility     0 negative     16\n 2 Sense & Sensibility     0 positive     26\n 3 Sense & Sensibility     1 negative     19\n 4 Sense & Sensibility     1 positive     44\n 5 Sense & Sensibility     2 negative     12\n 6 Sense & Sensibility     2 positive     23\n 7 Sense & Sensibility     3 negative     15\n 8 Sense & Sensibility     3 positive     22\n 9 Sense & Sensibility     4 negative     16\n10 Sense & Sensibility     4 positive     29\n# ℹ 1,830 more rows\n```\n:::\n:::\n\n\n::: callout-tip\n### Note\n\nThe `%/%` operator does **integer division** (`x %/% y` is equivalent to `floor(x/y)`) so the index keeps track of which 80-line section of text we are counting up negative and positive sentiment in.\n:::\n\nFinally, we use `pivot_wider()` to have positive and negative counts in different columns, and then use `mutate()` to calculate a net sentiment (positive - negative).\n\n\n::: {.cell}\n\n```{.r .cell-code}\njane_austen_sentiment <-\n    tidy_books %>%\n    inner_join(get_sentiments(\"bing\")) %>%\n    count(book,\n        index = linenumber %/% 80,\n        sentiment\n    ) %>%\n    pivot_wider(\n        names_from = sentiment,\n        values_from = n,\n        values_fill = 0\n    ) %>%\n    mutate(sentiment = positive - negative)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining with `by = join_by(word)`\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in inner_join(., get_sentiments(\"bing\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 131015 of `x` matches multiple rows in `y`.\nℹ Row 5051 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n```\n:::\n\n```{.r .cell-code}\njane_austen_sentiment\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 920 × 5\n   book                index negative positive sentiment\n   <fct>               <dbl>    <int>    <int>     <int>\n 1 Sense & Sensibility     0       16       26        10\n 2 Sense & Sensibility     1       19       44        25\n 3 Sense & Sensibility     2       12       23        11\n 4 Sense & Sensibility     3       15       22         7\n 5 Sense & Sensibility     4       16       29        13\n 6 Sense & Sensibility     5       16       39        23\n 7 Sense & Sensibility     6       24       37        13\n 8 Sense & Sensibility     7       22       39        17\n 9 Sense & Sensibility     8       30       35         5\n10 Sense & Sensibility     9       14       18         4\n# ℹ 910 more rows\n```\n:::\n:::\n\n\nThen we can plot the sentiment scores across the sections of each novel:\n\n\n::: {.cell}\n\n```{.r .cell-code}\njane_austen_sentiment %>%\n    ggplot(aes(x = index, y = sentiment, fill = book)) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(. ~ book, ncol = 2, scales = \"free_x\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-37-1.png){width=672}\n:::\n:::\n\n\nWe can see how the sentiment trajectory of the novel changes over time.\n\n### Word clouds\n\nYou can also do things like create word clouds using the `wordcloud` package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(wordcloud)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: RColorBrewer\n```\n:::\n\n```{.r .cell-code}\ntidy_books %>%\n    anti_join(stop_words) %>%\n    count(word) %>%\n    with(wordcloud(word, n, max.words = 100))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining with `by = join_by(word)`\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in wordcloud(word, n, max.words = 100): miss could not be fit on page.\nIt will not be plotted.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-38-1.png){width=672}\n:::\n:::\n\n\n# Converting to and from tidy and non-tidy formats\n\nIn this section, we want to **convert our tidy text data** constructed with the `unnest_tokens()` function (useable by packages in the tidyverse) into a different format that can be **used by packages for natural language processing** or other types of machine learning algorithms in non-tidy formats.\n\nIn the figure below, we see how an analysis might switch between tidy and non-tidy data structures and tools.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![ A flowchart of a typical text analysis that combines tidytext with other tools and data formats, particularly the `tm` or `quanteda` packages. Here, we show how to convert back and forth between document-term matrices and tidy data frames, as well as converting from a Corpus object to a text data frame.](https://www.tidytextmining.com/images/tmwr_0501.png){width=90%}\n:::\n:::\n\n\n\\[[image source](https://www.tidytextmining.com/images/tmwr_0501.png)\\]\n\n<details>\n\n<summary>Click here for how to convert to and from tidy and non-tidy formats to build machine learning algorithms.</summary>\n\nTo introduce some of these tools, we first need to introduce **document-term matrices**, as well as **casting** a tidy data frame into a sparse matrix.\n\n### Document-term matrix\n\nOne of the most common structures that text mining packages work with is the **document-term matrix** (or DTM). This is a matrix where:\n\n-   each row represents one document (such as a book or article),\n-   each column represents one term, and\n-   each value (typically) contains the number of appearances of that term in that document.\n\nSince most pairings of document and term do not occur (they have the value zero), DTMs are usually implemented as sparse matrices.\n\nThese objects can be treated as though they were matrices (for example, accessing particular rows and columns), but are stored in a more efficient format.\n\nDTM objects **cannot be used directly with tidy tools**, just as tidy data frames cannot be used as input for most text mining packages. Thus, the `tidytext` package provides two verbs that convert between the two formats.\n\n-   `tidy()` turns a document-term matrix into a tidy data frame. This verb comes from the `broom` package, which provides similar tidying functions for many statistical models and objects.\n-   `cast()` turns a tidy one-term-per-row data frame into a matrix. `tidytext` provides three variations of this verb, each converting to a different type of matrix: `cast_sparse()` (converting to a sparse matrix from the `Matrix` package), `cast_dtm()` (converting to a `DocumentTermMatrix` object from `tm`), and `cast_dfm()` (converting to a `dfm` object from `quanteda`).\n\nA DTM is typically comparable to a tidy data frame after a count or a group_by/summarize that contains counts or another statistic for each combination of a term and document.\n\n### Creating DocumentTermMatrix objects\n\nPerhaps the most widely used implementation of DTMs in R is the `DocumentTermMatrix` class in the `tm` package. Many available text mining datasets are provided in this format.\n\nLet's create a sparse with `cast_sparse()` function and then a `dtm` with the `cast_dtm()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_austen <-\n    austen_books() %>%\n    mutate(line = row_number()) %>%\n    unnest_tokens(word, text) %>%\n    anti_join(stop_words)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining with `by = join_by(word)`\n```\n:::\n\n```{.r .cell-code}\ntidy_austen\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 217,609 × 3\n   book                 line word       \n   <fct>               <int> <chr>      \n 1 Sense & Sensibility     1 sense      \n 2 Sense & Sensibility     1 sensibility\n 3 Sense & Sensibility     3 jane       \n 4 Sense & Sensibility     3 austen     \n 5 Sense & Sensibility     5 1811       \n 6 Sense & Sensibility    10 chapter    \n 7 Sense & Sensibility    10 1          \n 8 Sense & Sensibility    13 family     \n 9 Sense & Sensibility    13 dashwood   \n10 Sense & Sensibility    13 settled    \n# ℹ 217,599 more rows\n```\n:::\n:::\n\n\nFirst, we'll make a sparse matrix with `cast_sparse(data, row, column, value)`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nausten_sparse <- tidy_austen %>%\n    count(line, word) %>%\n    cast_sparse(row = line, column = word, value = n)\n\nausten_sparse[1:10, 1:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n10 x 10 sparse Matrix of class \"dgCMatrix\"\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n  [[ suppressing 10 column names 'sense', 'sensibility', 'austen' ... ]]\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                      \n1  1 1 . . . . . . . .\n3  . . 1 1 . . . . . .\n5  . . . . 1 . . . . .\n10 . . . . . 1 1 . . .\n13 . . . . . . . 1 1 1\n14 . . . . . . . . . .\n15 . . . . . . . . . .\n16 . . . . . . . . . .\n17 . . . . . . . . 1 .\n18 . . . . . . . . . .\n```\n:::\n:::\n\n\nNext, we'll make a `dtm` object with `cast_dtm(data, document, matrix)`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nausten_dtm <- tidy_austen %>%\n    count(line, word) %>%\n    cast_dtm(document = line, term = word, value = n)\n\nausten_dtm\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<<DocumentTermMatrix (documents: 61010, terms: 13914)>>\nNon-/sparse entries: 216128/848677012\nSparsity           : 100%\nMaximal term length: 19\nWeighting          : term frequency (tf)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nclass(austen_dtm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"DocumentTermMatrix\"    \"simple_triplet_matrix\"\n```\n:::\n\n```{.r .cell-code}\ndim(austen_dtm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 61010 13914\n```\n:::\n\n```{.r .cell-code}\nas.matrix(austen_dtm[1:20, 1:10])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Terms\nDocs sense sensibility austen jane 1811 1 chapter dashwood estate family\n  1      1           1      0    0    0 0       0        0      0      0\n  3      0           0      1    1    0 0       0        0      0      0\n  5      0           0      0    0    1 0       0        0      0      0\n  10     0           0      0    0    0 1       1        0      0      0\n  13     0           0      0    0    0 0       0        1      1      1\n  14     0           0      0    0    0 0       0        0      0      0\n  15     0           0      0    0    0 0       0        0      0      0\n  16     0           0      0    0    0 0       0        0      0      0\n  17     0           0      0    0    0 0       0        0      1      0\n  18     0           0      0    0    0 0       0        0      0      0\n  19     0           0      0    0    0 0       0        0      0      0\n  20     0           0      0    0    0 0       0        0      0      0\n  21     0           0      0    0    0 0       0        0      0      0\n  22     0           0      0    0    0 0       0        1      0      1\n  23     0           0      0    0    0 0       0        0      1      0\n  24     0           0      0    0    0 0       0        0      0      0\n  25     0           0      0    0    0 0       0        0      0      0\n  26     0           0      0    0    0 0       0        0      0      0\n  27     0           0      0    0    0 0       0        1      0      0\n  28     0           0      0    0    0 0       0        0      0      0\n```\n:::\n:::\n\n\nNow we have 61010 observations and 13914 features.\n\nWith these matricies, you can start to leverage the NLP methods and software. For example, in text mining, we often have collections of documents, such as blog posts or news articles, that we'd like to divide into natural groups so that we can understand them separately.\n\n**Topic modeling** is a method for **unsupervised classification** of such documents, similar to clustering on numeric data, which finds natural groups of items even when we're not sure what we are looking for.\n\nLatent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model. It treats each document as a mixture of topics, and each topic as a mixture of words. This allows documents to \"overlap\" each other in terms of content, rather than being separated into discrete groups, in a way that mirrors typical use of natural language.\n\nWe can also perform **supervised analyses** to build a classifier to classify lines of text from our `austen_sparse` or `austen_dtm` objects.\n\n</details>\n\n# R session information\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(width = 120)\nsessioninfo::session_info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n─ Session info ───────────────────────────────────────────────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Ventura 13.6\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-10-12\n pandoc   3.1.5 @ /opt/homebrew/bin/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────────────────────────────────────────────\n package      * version date (UTC) lib source\n cli            3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n colorout       1.3-0   2023-09-28 [1] Github (jalvesaq/colorout@8384882)\n colorspace     2.1-0   2023-01-23 [1] CRAN (R 4.3.0)\n digest         0.6.33  2023-07-07 [1] CRAN (R 4.3.0)\n dplyr        * 1.1.3   2023-09-03 [1] CRAN (R 4.3.0)\n evaluate       0.21    2023-05-05 [1] CRAN (R 4.3.0)\n fansi          1.0.4   2023-01-22 [1] CRAN (R 4.3.0)\n farver         2.1.1   2022-07-06 [1] CRAN (R 4.3.0)\n fastmap        1.1.1   2023-02-24 [1] CRAN (R 4.3.0)\n forcats      * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n fs             1.6.3   2023-07-20 [1] CRAN (R 4.3.0)\n generics       0.1.3   2022-07-05 [1] CRAN (R 4.3.0)\n ggplot2      * 3.4.3   2023-08-14 [1] CRAN (R 4.3.0)\n glue           1.6.2   2022-02-24 [1] CRAN (R 4.3.0)\n gtable         0.3.4   2023-08-21 [1] CRAN (R 4.3.0)\n hms            1.1.3   2023-03-21 [1] CRAN (R 4.3.0)\n htmltools      0.5.6   2023-08-10 [1] CRAN (R 4.3.0)\n htmlwidgets    1.6.2   2023-03-17 [1] CRAN (R 4.3.0)\n janeaustenr  * 1.0.0   2022-08-26 [1] CRAN (R 4.3.0)\n jsonlite       1.8.7   2023-06-29 [1] CRAN (R 4.3.0)\n knitr          1.44    2023-09-11 [1] CRAN (R 4.3.0)\n labeling       0.4.3   2023-08-29 [1] CRAN (R 4.3.0)\n lattice        0.21-8  2023-04-05 [1] CRAN (R 4.3.1)\n lifecycle      1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n lubridate    * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n magrittr       2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n Matrix         1.6-1   2023-08-14 [1] CRAN (R 4.3.0)\n munsell        0.5.0   2018-06-12 [1] CRAN (R 4.3.0)\n NLP            0.2-1   2020-10-14 [1] CRAN (R 4.3.0)\n pillar         1.9.0   2023-03-22 [1] CRAN (R 4.3.0)\n pkgconfig      2.0.3   2019-09-22 [1] CRAN (R 4.3.0)\n purrr        * 1.0.2   2023-08-10 [1] CRAN (R 4.3.0)\n R6             2.5.1   2021-08-19 [1] CRAN (R 4.3.0)\n rappdirs       0.3.3   2021-01-31 [1] CRAN (R 4.3.0)\n RColorBrewer * 1.1-3   2022-04-03 [1] CRAN (R 4.3.0)\n Rcpp           1.0.11  2023-07-06 [1] CRAN (R 4.3.0)\n readr        * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n rlang          1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown      2.24    2023-08-14 [1] CRAN (R 4.3.1)\n rstudioapi     0.15.0  2023-07-07 [1] CRAN (R 4.3.0)\n scales         1.2.1   2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo    1.2.2   2021-12-06 [1] CRAN (R 4.3.0)\n slam           0.1-50  2022-01-08 [1] CRAN (R 4.3.0)\n SnowballC      0.7.1   2023-04-25 [1] CRAN (R 4.3.0)\n stringi        1.7.12  2023-01-11 [1] CRAN (R 4.3.0)\n stringr      * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n textdata       0.4.4   2022-09-02 [1] CRAN (R 4.3.0)\n tibble       * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr        * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyselect     1.2.0   2022-10-10 [1] CRAN (R 4.3.0)\n tidytext     * 0.4.1   2023-01-07 [1] CRAN (R 4.3.0)\n tidyverse    * 2.0.0   2023-02-22 [1] CRAN (R 4.3.0)\n timechange     0.2.0   2023-01-11 [1] CRAN (R 4.3.0)\n tm             0.7-11  2023-02-05 [1] CRAN (R 4.3.0)\n tokenizers     0.3.0   2022-12-22 [1] CRAN (R 4.3.0)\n tzdb           0.4.0   2023-05-12 [1] CRAN (R 4.3.0)\n utf8           1.2.3   2023-01-31 [1] CRAN (R 4.3.0)\n vctrs          0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n withr          2.5.0   2022-03-03 [1] CRAN (R 4.3.0)\n wordcloud    * 2.6     2018-08-24 [1] CRAN (R 4.3.0)\n xfun           0.40    2023-08-09 [1] CRAN (R 4.3.0)\n xml2           1.3.5   2023-07-06 [1] CRAN (R 4.3.0)\n yaml           2.3.7   2023-01-23 [1] CRAN (R 4.3.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n```\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}